{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTM_W2v.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfA15bWrpL2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a280d3e-c9ac-49d2-fb37-c8b55dfdb681"
      },
      "source": [
        "import gensim\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model \n",
        "from IPython.display import Image\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(1003)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_5TPWcwp1Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W2V_DIR = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "DATA_DIR = ''\n",
        "Glove = 'glove.6B.200d.txt'\n",
        "\n",
        "# These are some hyperparameters that can be tuned\n",
        "MAX_SENT_LEN = 170\n",
        "MAX_VOCAB_SIZE = 400000\n",
        "LSTM_DIM = 128\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 200\n",
        "N_EPOCHS = 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G2ddghtqByf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bodies = pd.read_csv(DATA_DIR+'train_bodies.csv')\n",
        "train_stances = pd.read_csv(DATA_DIR+'train_stances.csv')\n",
        "\n",
        "test_bodies = pd.read_csv(DATA_DIR+'test_bodies.csv')\n",
        "test_stances_unlabeled = pd.read_csv(DATA_DIR+'test_stances_unlabeled.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eai0k018qPwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train_stances.join(train_bodies.set_index('Body ID'), on='Body ID')\n",
        "test = test_stances_unlabeled.join(test_bodies.set_index('Body ID'), on='Body ID')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uvg0bL4qTgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.replace('unrelated',1,True)\n",
        "train.replace('agree',2,True)\n",
        "train.replace('disagree',3,True)\n",
        "train.replace('discuss',4,True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoKVdcwoqV3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_seq_headline_train = [text_to_word_sequence(sent) for sent in train['Headline']]\n",
        "word_seq_bodies_train = [text_to_word_sequence(sent) for sent in train['articleBody']]\n",
        "\n",
        "word_seq_headline_test = [text_to_word_sequence(sent) for sent in test['Headline']]\n",
        "word_seq_bodies_test = [text_to_word_sequence(sent) for sent in test['articleBody']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olWALbv1qetn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_seq = []\n",
        "for i in range(len(word_seq_headline_train)):\n",
        "  word_seq.append(word_seq_headline_train[i])\n",
        "  \n",
        "for i in range(len(word_seq_bodies_train)):\n",
        "  word_seq.append(word_seq_bodies_train[i])\n",
        "\n",
        "for i in range(len(word_seq_headline_test)):\n",
        "  word_seq.append(word_seq_headline_test[i])\n",
        "\n",
        "for i in range(len(word_seq_bodies_test)):\n",
        "  word_seq.append(word_seq_bodies_test[i])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEA86DdvqjZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (len(word_seq_headline_train)):\n",
        "  word_seq_headline_train[i].extend(word_seq_bodies_train[i])\n",
        "\n",
        "  \n",
        "for i in range (len(word_seq_headline_test)):\n",
        "  word_seq_headline_test[i].extend(word_seq_bodies_test[i])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzgMnX99qm8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfDi_Ir4qrMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the sequence of words to sequnce of indices\n",
        "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_train])\n",
        "X_train = pad_sequences(X_train, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "y_train = train['Stance']\n",
        "y_train = y_train.values\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsYV4QWHquLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "encoder_train = LabelEncoder()\n",
        "encoder_train.fit(y_train)\n",
        "encoded_train = encoder_train.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y_train = np_utils.to_categorical(encoded_train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG-Q5zXvqwvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, dummy_y_train, random_state=10, test_size=0.1)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPBaB6Plq0NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(W2V_DIR, binary=True)\n",
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
        "\n",
        "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
        "    try:\n",
        "        embeddings_vector = embeddings[word]\n",
        "    except KeyError:\n",
        "        embeddings_vector = None\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[i] = embeddings_vector\n",
        "        \n",
        "del embeddings"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVnyoDHhq96v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "a37d20b3-46d7-4fd7-a8ae-7e90092500da"
      },
      "source": [
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Conv1D, MaxPooling1D\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "model_conv = Sequential()\n",
        "model_conv.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer'\n",
        "                          ))\n",
        "model_conv.add(Dropout(0.2))\n",
        "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
        "model_conv.add(MaxPooling1D(pool_size=4))\n",
        "model_conv.add(LSTM(100))\n",
        "model_conv.add(Dense(4, activation='sigmoid'))\n",
        "model_conv.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "model_conv.summary()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "word_embedding_layer (Embedd (None, None, 300)         6714300   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 64)          96064     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               66000     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 6,876,768\n",
            "Trainable params: 162,468\n",
            "Non-trainable params: 6,714,300\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMM_vXNbSgWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "ebeb2f08-7657-430e-b524-ca1679b0d9ec"
      },
      "source": [
        "history=model_conv.fit(X_train, y_train, batch_size=100, epochs=20, verbose=1, validation_data=(X_val, y_val))  # starts training\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 44974 samples, validate on 4998 samples\n",
            "Epoch 1/20\n",
            "44974/44974 [==============================] - 145s 3ms/step - loss: 0.7748 - accuracy: 0.7305 - val_loss: 0.7177 - val_accuracy: 0.7291\n",
            "Epoch 2/20\n",
            "44974/44974 [==============================] - 142s 3ms/step - loss: 0.6528 - accuracy: 0.7583 - val_loss: 0.5793 - val_accuracy: 0.7893\n",
            "Epoch 3/20\n",
            "44974/44974 [==============================] - 141s 3ms/step - loss: 0.5241 - accuracy: 0.8080 - val_loss: 0.5244 - val_accuracy: 0.7989\n",
            "Epoch 4/20\n",
            "44974/44974 [==============================] - 146s 3ms/step - loss: 0.4463 - accuracy: 0.8295 - val_loss: 0.4626 - val_accuracy: 0.8123\n",
            "Epoch 5/20\n",
            "44974/44974 [==============================] - 143s 3ms/step - loss: 0.3893 - accuracy: 0.8494 - val_loss: 0.4127 - val_accuracy: 0.8407\n",
            "Epoch 6/20\n",
            "44974/44974 [==============================] - 143s 3ms/step - loss: 0.3476 - accuracy: 0.8633 - val_loss: 0.3882 - val_accuracy: 0.8377\n",
            "Epoch 7/20\n",
            "44974/44974 [==============================] - 142s 3ms/step - loss: 0.3178 - accuracy: 0.8751 - val_loss: 0.3704 - val_accuracy: 0.8543\n",
            "Epoch 8/20\n",
            "44974/44974 [==============================] - 143s 3ms/step - loss: 0.2890 - accuracy: 0.8861 - val_loss: 0.3664 - val_accuracy: 0.8595\n",
            "Epoch 9/20\n",
            "44974/44974 [==============================] - 143s 3ms/step - loss: 0.2646 - accuracy: 0.8961 - val_loss: 0.3512 - val_accuracy: 0.8593\n",
            "Epoch 10/20\n",
            "44974/44974 [==============================] - 140s 3ms/step - loss: 0.2424 - accuracy: 0.9056 - val_loss: 0.3224 - val_accuracy: 0.8729\n",
            "Epoch 11/20\n",
            "44974/44974 [==============================] - 140s 3ms/step - loss: 0.2219 - accuracy: 0.9135 - val_loss: 0.3262 - val_accuracy: 0.8709\n",
            "Epoch 12/20\n",
            "44974/44974 [==============================] - 141s 3ms/step - loss: 0.2038 - accuracy: 0.9196 - val_loss: 0.3004 - val_accuracy: 0.8836\n",
            "Epoch 13/20\n",
            "44974/44974 [==============================] - 143s 3ms/step - loss: 0.1854 - accuracy: 0.9286 - val_loss: 0.3054 - val_accuracy: 0.8808\n",
            "Epoch 14/20\n",
            "44974/44974 [==============================] - 141s 3ms/step - loss: 0.1735 - accuracy: 0.9324 - val_loss: 0.2758 - val_accuracy: 0.8952\n",
            "Epoch 15/20\n",
            "44974/44974 [==============================] - 140s 3ms/step - loss: 0.1606 - accuracy: 0.9382 - val_loss: 0.2658 - val_accuracy: 0.8982\n",
            "Epoch 16/20\n",
            "44974/44974 [==============================] - 141s 3ms/step - loss: 0.1491 - accuracy: 0.9413 - val_loss: 0.2792 - val_accuracy: 0.8942\n",
            "Epoch 17/20\n",
            "44974/44974 [==============================] - 141s 3ms/step - loss: 0.1384 - accuracy: 0.9459 - val_loss: 0.2731 - val_accuracy: 0.9018\n",
            "Epoch 18/20\n",
            "44974/44974 [==============================] - 140s 3ms/step - loss: 0.1309 - accuracy: 0.9493 - val_loss: 0.2630 - val_accuracy: 0.9070\n",
            "Epoch 19/20\n",
            "44974/44974 [==============================] - 140s 3ms/step - loss: 0.1240 - accuracy: 0.9518 - val_loss: 0.2393 - val_accuracy: 0.9130\n",
            "Epoch 20/20\n",
            "44974/44974 [==============================] - 141s 3ms/step - loss: 0.1148 - accuracy: 0.9563 - val_loss: 0.2572 - val_accuracy: 0.9160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pwIzN5rEKgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('LSTMCNN_W2v', 'wb') as file_pi:\n",
        "  pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk_F9TPW5H_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31591590-c911-465f-e759-dd8798ff01f9"
      },
      "source": [
        "competetion_bodies = pd.read_csv(DATA_DIR+'competition_test_bodies.csv')\n",
        "competetion_stances = pd.read_csv(DATA_DIR+'competition_test_stances.csv')\n",
        "\n",
        "competetion_unlabeled = pd.read_csv(DATA_DIR+'competition_test_stances_unlabeled.csv')\n",
        "\n",
        "comp = competetion_stances.join(competetion_bodies.set_index('Body ID'), on='Body ID')\n",
        "comp.replace('unrelated',1,True)\n",
        "comp.replace('agree',2,True)\n",
        "comp.replace('disagree',3,True)\n",
        "comp.replace('discuss',4,True)\n",
        "\n",
        "word_seq_headline_comp = [text_to_word_sequence(sent) for sent in comp['Headline']]\n",
        "word_seq_bodies_comp = [text_to_word_sequence(sent) for sent in comp['articleBody']]\n",
        "\n",
        "for i in range (len(word_seq_headline_comp)):\n",
        "  word_seq_headline_comp[i].extend(word_seq_bodies_comp[i])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_comp])\n",
        "\n",
        "X_comp = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_comp])\n",
        "X_comp = pad_sequences(X_comp, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "y_comp = comp['Stance']\n",
        "y_comp = y_comp.values\n",
        "\n",
        "encoder_comp = LabelEncoder()\n",
        "encoder_comp.fit(y_comp)\n",
        "encoded_comp = encoder_comp.transform(y_comp)\n",
        "dummy_y_comp = np_utils.to_categorical(encoded_comp)\n",
        "\n",
        "y_Uni = model_conv.predict(X_comp)\n",
        "score,acc = model_conv.evaluate(X_comp, dummy_y_comp)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25413/25413 [==============================] - 26s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxyKiIRw5t0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c0dc01c9-afcc-4cdc-8860-48fd90fdf390"
      },
      "source": [
        "from score import report_score\n",
        "outputs = [np.argmax(p) for p in y_Uni]\n",
        "\n",
        "for i in range(len(outputs)):\n",
        "    if outputs[i] == 0: outputs[i] = \"unrelated\"\n",
        "    if outputs[i] == 1: outputs[i] = \"disagree\"\n",
        "    if outputs[i] == 2: outputs[i] = \"agree\"\n",
        "    if outputs[i] == 3: outputs[i] = \"discuss\"\n",
        "#print (np.unique(outputs))\n",
        "\n",
        "cs = pd.read_csv(DATA_DIR+'competition_test_stances.csv')\n",
        "stance_true = cs['Stance'].values\n",
        "from score import report_score\n",
        "print (\"Weighted Score\")\n",
        "report_score(stance_true, outputs)\n",
        "\n",
        "Predicted = {}\n",
        "Predicted = pd.DataFrame({'Stance': outputs})\n",
        "result = pd.concat([competetion_unlabeled, Predicted], axis=1, sort=False)\n",
        "result.to_csv('submission_LSTM_Glove.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weighted Score\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |     1     |    59     |    311    |   1532    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |     0     |     8     |    77     |    612    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |     8     |    193    |    698    |   3565    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |    30     |    423    |   2200    |   15696   |\n",
            "-------------------------------------------------------------\n",
            "Score: 4793.0 out of 11651.25\t(41.137217036798624%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}