{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNLSTM_glove.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfA15bWrpL2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a31d2c8-6aac-4c3f-dfe0-081f1212ef2e"
      },
      "source": [
        "import gensim\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model \n",
        "from IPython.display import Image\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(1003)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_5TPWcwp1Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W2V_DIR = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "DATA_DIR = ''\n",
        "Glove = 'glove.6B.100d.txt'\n",
        "\n",
        "# These are some hyperparameters that can be tuned\n",
        "MAX_SENT_LEN = 170\n",
        "MAX_VOCAB_SIZE = 400000\n",
        "LSTM_DIM = 128\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 200\n",
        "N_EPOCHS = 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G2ddghtqByf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_bodies = pd.read_csv(DATA_DIR+'train_bodies.csv')\n",
        "train_stances = pd.read_csv(DATA_DIR+'train_stances.csv')\n",
        "\n",
        "test_bodies = pd.read_csv(DATA_DIR+'test_bodies.csv')\n",
        "test_stances_unlabeled = pd.read_csv(DATA_DIR+'test_stances_unlabeled.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eai0k018qPwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train_stances.join(train_bodies.set_index('Body ID'), on='Body ID')\n",
        "test = test_stances_unlabeled.join(test_bodies.set_index('Body ID'), on='Body ID')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uvg0bL4qTgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.replace('unrelated',1,True)\n",
        "train.replace('agree',2,True)\n",
        "train.replace('disagree',3,True)\n",
        "train.replace('discuss',4,True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoKVdcwoqV3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_seq_headline_train = [text_to_word_sequence(sent) for sent in train['Headline']]\n",
        "word_seq_bodies_train = [text_to_word_sequence(sent) for sent in train['articleBody']]\n",
        "\n",
        "word_seq_headline_test = [text_to_word_sequence(sent) for sent in test['Headline']]\n",
        "word_seq_bodies_test = [text_to_word_sequence(sent) for sent in test['articleBody']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olWALbv1qetn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_seq = []\n",
        "for i in range(len(word_seq_headline_train)):\n",
        "  word_seq.append(word_seq_headline_train[i])\n",
        "  \n",
        "for i in range(len(word_seq_bodies_train)):\n",
        "  word_seq.append(word_seq_bodies_train[i])\n",
        "\n",
        "for i in range(len(word_seq_headline_test)):\n",
        "  word_seq.append(word_seq_headline_test[i])\n",
        "\n",
        "for i in range(len(word_seq_bodies_test)):\n",
        "  word_seq.append(word_seq_bodies_test[i])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEA86DdvqjZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (len(word_seq_headline_train)):\n",
        "  word_seq_headline_train[i].extend(word_seq_bodies_train[i])\n",
        "\n",
        "  \n",
        "for i in range (len(word_seq_headline_test)):\n",
        "  word_seq_headline_test[i].extend(word_seq_bodies_test[i])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzgMnX99qm8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfDi_Ir4qrMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the sequence of words to sequnce of indices\n",
        "X_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_train])\n",
        "X_train = pad_sequences(X_train, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "y_train = train['Stance']\n",
        "y_train = y_train.values\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsYV4QWHquLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "encoder_train = LabelEncoder()\n",
        "encoder_train.fit(y_train)\n",
        "encoded_train = encoder_train.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y_train = np_utils.to_categorical(encoded_train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG-Q5zXvqwvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, dummy_y_train, random_state=10, test_size=0.1)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPBaB6Plq0NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_input_file=\"glove.6B.100d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
        "\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n",
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
        "\n",
        "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
        "    try:\n",
        "        embeddings_vector = embeddings[word]\n",
        "    except KeyError:\n",
        "        embeddings_vector = None\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[i] = embeddings_vector\n",
        "        \n",
        "del embeddings"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVnyoDHhq96v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "c6bdf92d-1b63-4b3f-8a88-ddc2e4b6fd83"
      },
      "source": [
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Conv1D, MaxPooling1D\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "model_conv = Sequential()\n",
        "model_conv.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer'\n",
        "                          ))\n",
        "model_conv.add(Dropout(0.2))\n",
        "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
        "model_conv.add(MaxPooling1D(pool_size=4))\n",
        "model_conv.add(LSTM(100))\n",
        "model_conv.add(Dense(4, activation='sigmoid'))\n",
        "model_conv.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "model_conv.summary()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "word_embedding_layer (Embedd (None, None, 100)         2238100   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 64)          32064     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               66000     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 2,336,568\n",
            "Trainable params: 98,468\n",
            "Non-trainable params: 2,238,100\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMM_vXNbSgWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "87f2dafc-4268-47be-c625-b8b3af490260"
      },
      "source": [
        "history=model_conv.fit(X_train, y_train, batch_size=100, epochs=20, verbose=1, validation_data=(X_val, y_val))  # starts training\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 44974 samples, validate on 4998 samples\n",
            "Epoch 1/20\n",
            "44974/44974 [==============================] - 83s 2ms/step - loss: 0.7855 - accuracy: 0.7301 - val_loss: 0.7949 - val_accuracy: 0.7291\n",
            "Epoch 2/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.7422 - accuracy: 0.7328 - val_loss: 0.6990 - val_accuracy: 0.7511\n",
            "Epoch 3/20\n",
            "44974/44974 [==============================] - 80s 2ms/step - loss: 0.6661 - accuracy: 0.7657 - val_loss: 0.6062 - val_accuracy: 0.7827\n",
            "Epoch 4/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.5599 - accuracy: 0.7944 - val_loss: 0.5329 - val_accuracy: 0.8011\n",
            "Epoch 5/20\n",
            "44974/44974 [==============================] - 80s 2ms/step - loss: 0.4805 - accuracy: 0.8187 - val_loss: 0.4578 - val_accuracy: 0.8227\n",
            "Epoch 6/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.4282 - accuracy: 0.8359 - val_loss: 0.4337 - val_accuracy: 0.8351\n",
            "Epoch 7/20\n",
            "44974/44974 [==============================] - 83s 2ms/step - loss: 0.3862 - accuracy: 0.8497 - val_loss: 0.3941 - val_accuracy: 0.8441\n",
            "Epoch 8/20\n",
            "44974/44974 [==============================] - 83s 2ms/step - loss: 0.3571 - accuracy: 0.8618 - val_loss: 0.3703 - val_accuracy: 0.8547\n",
            "Epoch 9/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.3336 - accuracy: 0.8736 - val_loss: 0.3544 - val_accuracy: 0.8543\n",
            "Epoch 10/20\n",
            "44974/44974 [==============================] - 83s 2ms/step - loss: 0.3130 - accuracy: 0.8790 - val_loss: 0.3351 - val_accuracy: 0.8657\n",
            "Epoch 11/20\n",
            "44974/44974 [==============================] - 82s 2ms/step - loss: 0.2917 - accuracy: 0.8888 - val_loss: 0.3110 - val_accuracy: 0.8816\n",
            "Epoch 12/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.2716 - accuracy: 0.8959 - val_loss: 0.2935 - val_accuracy: 0.8856\n",
            "Epoch 13/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.2594 - accuracy: 0.9021 - val_loss: 0.2802 - val_accuracy: 0.8936\n",
            "Epoch 14/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.2430 - accuracy: 0.9073 - val_loss: 0.2807 - val_accuracy: 0.8920\n",
            "Epoch 15/20\n",
            "44974/44974 [==============================] - 83s 2ms/step - loss: 0.2321 - accuracy: 0.9118 - val_loss: 0.2519 - val_accuracy: 0.9054\n",
            "Epoch 16/20\n",
            "44974/44974 [==============================] - 80s 2ms/step - loss: 0.2156 - accuracy: 0.9174 - val_loss: 0.2724 - val_accuracy: 0.8940\n",
            "Epoch 17/20\n",
            "44974/44974 [==============================] - 79s 2ms/step - loss: 0.2101 - accuracy: 0.9195 - val_loss: 0.2437 - val_accuracy: 0.9102\n",
            "Epoch 18/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.1970 - accuracy: 0.9257 - val_loss: 0.2550 - val_accuracy: 0.9008\n",
            "Epoch 19/20\n",
            "44974/44974 [==============================] - 82s 2ms/step - loss: 0.1897 - accuracy: 0.9280 - val_loss: 0.2361 - val_accuracy: 0.9114\n",
            "Epoch 20/20\n",
            "44974/44974 [==============================] - 81s 2ms/step - loss: 0.1820 - accuracy: 0.9309 - val_loss: 0.2199 - val_accuracy: 0.9204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pwIzN5rEKgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('LSTMCNN_glove', 'wb') as file_pi:\n",
        "  pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk_F9TPW5H_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fbdd969-b0ca-41e4-d9d5-7b839897b689"
      },
      "source": [
        "competetion_bodies = pd.read_csv(DATA_DIR+'competition_test_bodies.csv')\n",
        "competetion_stances = pd.read_csv(DATA_DIR+'competition_test_stances.csv')\n",
        "\n",
        "competetion_unlabeled = pd.read_csv(DATA_DIR+'competition_test_stances_unlabeled.csv')\n",
        "\n",
        "comp = competetion_stances.join(competetion_bodies.set_index('Body ID'), on='Body ID')\n",
        "comp.replace('unrelated',1,True)\n",
        "comp.replace('agree',2,True)\n",
        "comp.replace('disagree',3,True)\n",
        "comp.replace('discuss',4,True)\n",
        "\n",
        "word_seq_headline_comp = [text_to_word_sequence(sent) for sent in comp['Headline']]\n",
        "word_seq_bodies_comp = [text_to_word_sequence(sent) for sent in comp['articleBody']]\n",
        "\n",
        "for i in range (len(word_seq_headline_comp)):\n",
        "  word_seq_headline_comp[i].extend(word_seq_bodies_comp[i])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_comp])\n",
        "\n",
        "X_comp = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in word_seq_headline_comp])\n",
        "X_comp = pad_sequences(X_comp, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "y_comp = comp['Stance']\n",
        "y_comp = y_comp.values\n",
        "\n",
        "encoder_comp = LabelEncoder()\n",
        "encoder_comp.fit(y_comp)\n",
        "encoded_comp = encoder_comp.transform(y_comp)\n",
        "dummy_y_comp = np_utils.to_categorical(encoded_comp)\n",
        "\n",
        "y_Uni = model_conv.predict(X_comp)\n",
        "score,acc = model_conv.evaluate(X_comp, dummy_y_comp)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25413/25413 [==============================] - 14s 551us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxyKiIRw5t0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "bf3d9e17-96a6-43e2-c43d-90133913fa22"
      },
      "source": [
        "from score import report_score\n",
        "outputs = [np.argmax(p) for p in y_Uni]\n",
        "\n",
        "for i in range(len(outputs)):\n",
        "    if outputs[i] == 0: outputs[i] = \"unrelated\"\n",
        "    if outputs[i] == 1: outputs[i] = \"disagree\"\n",
        "    if outputs[i] == 2: outputs[i] = \"agree\"\n",
        "    if outputs[i] == 3: outputs[i] = \"discuss\"\n",
        "#print (np.unique(outputs))\n",
        "\n",
        "cs = pd.read_csv(DATA_DIR+'competition_test_stances.csv')\n",
        "stance_true = cs['Stance'].values\n",
        "from score import report_score\n",
        "print (\"Weighted Score\")\n",
        "report_score(stance_true, outputs)\n",
        "\n",
        "Predicted = {}\n",
        "Predicted = pd.DataFrame({'Stance': outputs})\n",
        "result = pd.concat([competetion_unlabeled, Predicted], axis=1, sort=False)\n",
        "result.to_csv('submission_LSTMCNN_Glove.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weighted Score\n",
            "-------------------------------------------------------------\n",
            "|           |   agree   | disagree  |  discuss  | unrelated |\n",
            "-------------------------------------------------------------\n",
            "|   agree   |    13     |    183    |    164    |   1543    |\n",
            "-------------------------------------------------------------\n",
            "| disagree  |     0     |    45     |    57     |    595    |\n",
            "-------------------------------------------------------------\n",
            "|  discuss  |     5     |    501    |    429    |   3529    |\n",
            "-------------------------------------------------------------\n",
            "| unrelated |    40     |   1410    |   1856    |   15043   |\n",
            "-------------------------------------------------------------\n",
            "Score: 4475.25 out of 11651.25\t(38.41004184100419%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}